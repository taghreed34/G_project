{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# pylint: disable=g-bad-import-order\n",
    "from absl import flags\n",
    "import tensorflow as tf\n",
    "\n",
    "from official import resnet_model\n",
    "########################\n",
    "# Functions for input processing.\n",
    "################################################################################\n",
    "def process_record_dataset(dataset,\n",
    "                           is_training,\n",
    "                           batch_size,\n",
    "                           shuffle_buffer,\n",
    "                           parse_record_fn,\n",
    "                           num_epochs=1,\n",
    "                           dtype=tf.float32,\n",
    "                           datasets_num_private_threads=None,\n",
    "                           num_parallel_batches=1):\n",
    "  \"\"\"Given a Dataset with raw records, return an iterator over the records.\n",
    "  Args:\n",
    "    dataset: A Dataset representing raw records\n",
    "    is_training: A boolean denoting whether the input is for training.\n",
    "    batch_size: The number of samples per batch.\n",
    "    shuffle_buffer: The buffer size to use when shuffling records. A larger\n",
    "      value results in better randomness, but smaller values reduce startup\n",
    "      time and use less memory.\n",
    "    parse_record_fn: A function that takes a raw record and returns the\n",
    "      corresponding (image, label) pair.\n",
    "    num_epochs: The number of epochs to repeat the dataset.\n",
    "    dtype: Data type to use for images/features.\n",
    "    datasets_num_private_threads: Number of threads for a private\n",
    "      threadpool created for all datasets computation.\n",
    "    num_parallel_batches: Number of parallel batches for tf.data.\n",
    "  Returns:\n",
    "    Dataset of (image, label) pairs ready for iteration.\n",
    "  \"\"\"\n",
    "  # Defines a specific size thread pool for tf.data operations.\n",
    "  if datasets_num_private_threads:\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_threading.private_threadpool_size = (\n",
    "        datasets_num_private_threads)\n",
    "    dataset = dataset.with_options(options)\n",
    "    tf.compat.v1.logging.info('datasets_num_private_threads: %s',\n",
    "                              datasets_num_private_threads)\n",
    "\n",
    "  # Prefetches a batch at a time to smooth out the time taken to load input\n",
    "  # files for shuffling and processing.\n",
    "  dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "  if is_training:\n",
    "    # Shuffles records before repeating to respect epoch boundaries.\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
    "\n",
    "  # Repeats the dataset for the number of epochs to train.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "  # Parses the raw records into images and labels.\n",
    "  dataset = dataset.apply(\n",
    "      tf.data.experimental.map_and_batch(\n",
    "          lambda value: parse_record_fn(value, is_training, dtype),\n",
    "          batch_size=batch_size,\n",
    "          num_parallel_batches=num_parallel_batches,\n",
    "          drop_remainder=False))\n",
    "\n",
    "  # Operations between the final prefetch and the get_next call to the iterator\n",
    "  # will happen synchronously during run time. We prefetch here again to\n",
    "  # background all of the above processing work and keep it out of the\n",
    "  # critical training path. Setting buffer_size to tf.contrib.data.AUTOTUNE\n",
    "  # allows DistributionStrategies to adjust how many batches to fetch based\n",
    "  # on how many devices are present.\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_synth_input_fn(height, width, num_channels, num_classes,\n",
    "                       dtype=tf.float32):\n",
    "  \"\"\"Returns an input function that returns a dataset with random data.\n",
    "  This input_fn returns a data set that iterates over a set of random data and\n",
    "  bypasses all preprocessing, e.g. jpeg decode and copy. The host to device\n",
    "  copy is still included. This used to find the upper throughput bound when\n",
    "  tunning the full input pipeline.\n",
    "  Args:\n",
    "    height: Integer height that will be used to create a fake image tensor.\n",
    "    width: Integer width that will be used to create a fake image tensor.\n",
    "    num_channels: Integer depth that will be used to create a fake image tensor.\n",
    "    num_classes: Number of classes that should be represented in the fake labels\n",
    "      tensor\n",
    "    dtype: Data type for features/images.\n",
    "  Returns:\n",
    "    An input_fn that can be used in place of a real one to return a dataset\n",
    "    that can be used for iteration.\n",
    "  \"\"\"\n",
    "  # pylint: disable=unused-argument\n",
    "  def input_fn(is_training, data_dir, batch_size, *args, **kwargs):\n",
    "    \"\"\"Returns dataset filled with random data.\"\"\"\n",
    "    # Synthetic input should be within [0, 255].\n",
    "    inputs = tf.random.truncated_normal(\n",
    "        [batch_size] + [height, width, num_channels],\n",
    "        dtype=dtype,\n",
    "        mean=127,\n",
    "        stddev=60,\n",
    "        name='synthetic_inputs')\n",
    "\n",
    "    labels = tf.random.uniform(\n",
    "        [batch_size],\n",
    "        minval=0,\n",
    "        maxval=num_classes - 1,\n",
    "        dtype=tf.int32,\n",
    "        name='synthetic_labels')\n",
    "    data = tf.data.Dataset.from_tensors((inputs, labels)).repeat()\n",
    "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return data\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "def image_bytes_serving_input_fn(image_shape, dtype=tf.float32):\n",
    "  \"\"\"Serving input fn for raw jpeg images.\"\"\"\n",
    "\n",
    "  def _preprocess_image(image_bytes):\n",
    "    \"\"\"Preprocess a single raw image.\"\"\"\n",
    "    # Bounding box around the whole image.\n",
    "    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=dtype, shape=[1, 1, 4])\n",
    "    height, width, num_channels = image_shape\n",
    "    image = imagenet_preprocessing.preprocess_image(\n",
    "        image_bytes, bbox, height, width, num_channels, is_training=False)\n",
    "    return image\n",
    "\n",
    "  image_bytes_list = tf.compat.v1.placeholder(\n",
    "      shape=[None], dtype=tf.string, name='input_tensor')\n",
    "  images = tf.map_fn(\n",
    "      _preprocess_image, image_bytes_list, back_prop=False, dtype=dtype)\n",
    "  return tf.estimator.export.TensorServingInputReceiver(\n",
    "      images, {'image_bytes': image_bytes_list})\n",
    "\n",
    "\n",
    "def override_flags_and_set_envars_for_gpu_thread_pool(flags_obj):\n",
    "  \"\"\"Override flags and set env_vars for performance.\n",
    "  These settings exist to test the difference between using stock settings\n",
    "  and manual tuning. It also shows some of the ENV_VARS that can be tweaked to\n",
    "  squeeze a few extra examples per second.  These settings are defaulted to the\n",
    "  current platform of interest, which changes over time.\n",
    "  On systems with small numbers of cpu cores, e.g. under 8 logical cores,\n",
    "  setting up a gpu thread pool with `tf_gpu_thread_mode=gpu_private` may perform\n",
    "  poorly.\n",
    "  Args:\n",
    "    flags_obj: Current flags, which will be adjusted possibly overriding\n",
    "    what has been set by the user on the command-line.\n",
    "  \"\"\"\n",
    "  cpu_count = multiprocessing.cpu_count()\n",
    "  tf.compat.v1.logging.info('Logical CPU cores: %s', cpu_count)\n",
    "\n",
    "  # Sets up thread pool for each GPU for op scheduling.\n",
    "  per_gpu_thread_count = 1\n",
    "  total_gpu_thread_count = per_gpu_thread_count * flags_obj.num_gpus\n",
    "  os.environ['TF_GPU_THREAD_MODE'] = flags_obj.tf_gpu_thread_mode\n",
    "  os.environ['TF_GPU_THREAD_COUNT'] = str(per_gpu_thread_count)\n",
    "  tf.compat.v1.logging.info('TF_GPU_THREAD_COUNT: %s',\n",
    "                            os.environ['TF_GPU_THREAD_COUNT'])\n",
    "  tf.compat.v1.logging.info('TF_GPU_THREAD_MODE: %s',\n",
    "                            os.environ['TF_GPU_THREAD_MODE'])\n",
    "\n",
    "  # Reduces general thread pool by number of threads used for GPU pool.\n",
    "  main_thread_count = cpu_count - total_gpu_thread_count\n",
    "  flags_obj.inter_op_parallelism_threads = main_thread_count\n",
    "\n",
    "  # Sets thread count for tf.data. Logical cores minus threads assign to the\n",
    "  # private GPU pool along with 2 thread per GPU for event monitoring and\n",
    "  # sending / receiving tensors.\n",
    "  num_monitoring_threads = 2 * flags_obj.num_gpus\n",
    "  flags_obj.datasets_num_private_threads = (cpu_count - total_gpu_thread_count\n",
    "                                            - num_monitoring_threads)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Functions for running training/eval/validation loops for the model.\n",
    "################################################################################\n",
    "def learning_rate_with_decay(\n",
    "    batch_size, batch_denom, num_images, boundary_epochs, decay_rates,\n",
    "    base_lr=0.1, warmup=False):\n",
    "  \"\"\"Get a learning rate that decays step-wise as training progresses.\n",
    "  Args:\n",
    "    batch_size: the number of examples processed in each training batch.\n",
    "    batch_denom: this value will be used to scale the base learning rate.\n",
    "      `0.1 * batch size` is divided by this number, such that when\n",
    "      batch_denom == batch_size, the initial learning rate will be 0.1.\n",
    "    num_images: total number of images that will be used for training.\n",
    "    boundary_epochs: list of ints representing the epochs at which we\n",
    "      decay the learning rate.\n",
    "    decay_rates: list of floats representing the decay rates to be used\n",
    "      for scaling the learning rate. It should have one more element\n",
    "      than `boundary_epochs`, and all elements should have the same type.\n",
    "    base_lr: Initial learning rate scaled based on batch_denom.\n",
    "    warmup: Run a 5 epoch warmup to the initial lr.\n",
    "  Returns:\n",
    "    Returns a function that takes a single argument - the number of batches\n",
    "    trained so far (global_step)- and returns the learning rate to be used\n",
    "    for training the next batch.\n",
    "  \"\"\"\n",
    "  initial_learning_rate = base_lr * batch_size / batch_denom\n",
    "  batches_per_epoch = num_images / batch_size\n",
    "\n",
    "  # Reduce the learning rate at certain epochs.\n",
    "  # CIFAR-10: divide by 10 at epoch 100, 150, and 200\n",
    "  # ImageNet: divide by 10 at epoch 30, 60, 80, and 90\n",
    "  boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n",
    "  vals = [initial_learning_rate * decay for decay in decay_rates]\n",
    "\n",
    "  def learning_rate_fn(global_step):\n",
    "    \"\"\"Builds scaled learning rate function with 5 epoch warm up.\"\"\"\n",
    "    lr = tf.compat.v1.train.piecewise_constant(global_step, boundaries, vals)\n",
    "    if warmup:\n",
    "      warmup_steps = int(batches_per_epoch * 5)\n",
    "      warmup_lr = (\n",
    "          initial_learning_rate * tf.cast(global_step, tf.float32) / tf.cast(\n",
    "              warmup_steps, tf.float32))\n",
    "      return tf.cond(pred=global_step < warmup_steps,\n",
    "                     true_fn=lambda: warmup_lr,\n",
    "                     false_fn=lambda: lr)\n",
    "    return lr\n",
    "\n",
    "  return learning_rate_fn\n",
    "\n",
    "\n",
    "def resnet_model_fn(features, labels, mode, model_class,\n",
    "                    resnet_size, weight_decay, learning_rate_fn, momentum,\n",
    "                    data_format, resnet_version, loss_scale,\n",
    "                    loss_filter_fn=None, dtype=resnet_model.DEFAULT_DTYPE,\n",
    "                    fine_tune=False):\n",
    "  \"\"\"Shared functionality for different resnet model_fns.\n",
    "  Initializes the ResnetModel representing the model layers\n",
    "  and uses that model to build the necessary EstimatorSpecs for\n",
    "  the `mode` in question. For training, this means building losses,\n",
    "  the optimizer, and the train op that get passed into the EstimatorSpec.\n",
    "  For evaluation and prediction, the EstimatorSpec is returned without\n",
    "  a train op, but with the necessary parameters for the given mode.\n",
    "  Args:\n",
    "    features: tensor representing input images\n",
    "    labels: tensor representing class labels for all input images\n",
    "    mode: current estimator mode; should be one of\n",
    "      `tf.estimator.ModeKeys.TRAIN`, `EVALUATE`, `PREDICT`\n",
    "    model_class: a class representing a TensorFlow model that has a __call__\n",
    "      function. We assume here that this is a subclass of ResnetModel.\n",
    "    resnet_size: A single integer for the size of the ResNet model.\n",
    "    weight_decay: weight decay loss rate used to regularize learned variables.\n",
    "    learning_rate_fn: function that returns the current learning rate given\n",
    "      the current global_step\n",
    "    momentum: momentum term used for optimization\n",
    "    data_format: Input format ('channels_last', 'channels_first', or None).\n",
    "      If set to None, the format is dependent on whether a GPU is available.\n",
    "    resnet_version: Integer representing which version of the ResNet network to\n",
    "      use. See README for details. Valid values: [1, 2]\n",
    "    loss_scale: The factor to scale the loss for numerical stability. A detailed\n",
    "      summary is present in the arg parser help text.\n",
    "    loss_filter_fn: function that takes a string variable name and returns\n",
    "      True if the var should be included in loss calculation, and False\n",
    "      otherwise. If None, batch_normalization variables will be excluded\n",
    "      from the loss.\n",
    "    dtype: the TensorFlow dtype to use for calculations.\n",
    "    fine_tune: If True only train the dense layers(final layers).\n",
    "  Returns:\n",
    "    EstimatorSpec parameterized according to the input params and the\n",
    "    current mode.\n",
    "  \"\"\"\n",
    "\n",
    "  # Generate a summary node for the images\n",
    "  tf.compat.v1.summary.image('images', features, max_outputs=6)\n",
    "  # Checks that features/images have same data type being used for calculations.\n",
    "  assert features.dtype == dtype\n",
    "\n",
    "  model = model_class(resnet_size, data_format, resnet_version=resnet_version,\n",
    "                      dtype=dtype)\n",
    "\n",
    "  logits = model(features, mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # This acts as a no-op if the logits are already in fp32 (provided logits are\n",
    "  # not a SparseTensor). If dtype is is low precision, logits must be cast to\n",
    "  # fp32 for numerical stability.\n",
    "  logits = tf.cast(logits, tf.float32)\n",
    "\n",
    "  predictions = {\n",
    "      'classes': tf.argmax(input=logits, axis=1),\n",
    "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Return the predictions and the specification for serving a SavedModel\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        export_outputs={\n",
    "            'predict': tf.estimator.export.PredictOutput(predictions)\n",
    "        })\n",
    "\n",
    "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "  cross_entropy = tf.compat.v1.losses.sparse_softmax_cross_entropy(\n",
    "      logits=logits, labels=labels)\n",
    "\n",
    "  # Create a tensor named cross_entropy for logging purposes.\n",
    "  tf.identity(cross_entropy, name='cross_entropy')\n",
    "  tf.compat.v1.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  # If no loss_filter_fn is passed, assume we want the default behavior,\n",
    "  # which is that batch_normalization variables are excluded from loss.\n",
    "  def exclude_batch_norm(name):\n",
    "    return 'batch_normalization' not in name\n",
    "  loss_filter_fn = loss_filter_fn or exclude_batch_norm\n",
    "\n",
    "  # Add weight decay to the loss. We need to scale the regularization loss\n",
    "  # manually as losses other than in tf.losses and tf.keras.losses don't scale\n",
    "  # automatically.\n",
    "  l2_loss = weight_decay * tf.add_n(\n",
    "      # loss is computed using fp32 for numerical stability.\n",
    "      [\n",
    "          tf.nn.l2_loss(tf.cast(v, tf.float32))\n",
    "          for v in tf.compat.v1.trainable_variables()\n",
    "          if loss_filter_fn(v.name)\n",
    "      ]) / tf.distribute.get_strategy().num_replicas_in_sync\n",
    "  tf.compat.v1.summary.scalar('l2_loss', l2_loss)\n",
    "  loss = cross_entropy + l2_loss\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = learning_rate_fn(global_step)\n",
    "\n",
    "    # Create a tensor named learning_rate for logging purposes\n",
    "    tf.identity(learning_rate, name='learning_rate')\n",
    "    tf.compat.v1.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    optimizer = tf.compat.v1.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=momentum\n",
    "    )\n",
    "\n",
    "    def _dense_grad_filter(gvs):\n",
    "      \"\"\"Only apply gradient updates to the final layer.\n",
    "      This function is used for fine tuning.\n",
    "      Args:\n",
    "        gvs: list of tuples with gradients and variable info\n",
    "      Returns:\n",
    "        filtered gradients so that only the dense layer remains\n",
    "      \"\"\"\n",
    "      return [(g, v) for g, v in gvs if 'dense' in v.name]\n",
    "\n",
    "    if loss_scale != 1:\n",
    "      # When computing fp16 gradients, often intermediate tensor values are\n",
    "      # so small, they underflow to 0. To avoid this, we multiply the loss by\n",
    "      # loss_scale to make these tensor values loss_scale times bigger.\n",
    "      scaled_grad_vars = optimizer.compute_gradients(loss * loss_scale)\n",
    "\n",
    "      if fine_tune:\n",
    "        scaled_grad_vars = _dense_grad_filter(scaled_grad_vars)\n",
    "\n",
    "      # Once the gradient computation is complete we can scale the gradients\n",
    "      # back to the correct scale before passing them to the optimizer.\n",
    "      unscaled_grad_vars = [(grad / loss_scale, var)\n",
    "                            for grad, var in scaled_grad_vars]\n",
    "      minimize_op = optimizer.apply_gradients(unscaled_grad_vars, global_step)\n",
    "    else:\n",
    "      grad_vars = optimizer.compute_gradients(loss)\n",
    "      if fine_tune:\n",
    "        grad_vars = _dense_grad_filter(grad_vars)\n",
    "      minimize_op = optimizer.apply_gradients(grad_vars, global_step)\n",
    "\n",
    "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "    train_op = tf.group(minimize_op, update_ops)\n",
    "  else:\n",
    "    train_op = None\n",
    "\n",
    "  accuracy = tf.compat.v1.metrics.accuracy(labels, predictions['classes'])\n",
    "  accuracy_top_5 = tf.compat.v1.metrics.mean(\n",
    "      tf.nn.in_top_k(predictions=logits, targets=labels, k=5, name='top_5_op'))\n",
    "  metrics = {'accuracy': accuracy,\n",
    "             'accuracy_top_5': accuracy_top_5}\n",
    "\n",
    "  # Create a tensor named train_accuracy for logging purposes\n",
    "  tf.identity(accuracy[1], name='train_accuracy')\n",
    "  tf.identity(accuracy_top_5[1], name='train_accuracy_top_5')\n",
    "  tf.compat.v1.summary.scalar('train_accuracy', accuracy[1])\n",
    "  tf.compat.v1.summary.scalar('train_accuracy_top_5', accuracy_top_5[1])\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=metrics)\n",
    "\n",
    "\n",
    "def resnet_main(\n",
    "    flags_obj, model_function, input_function, dataset_name, shape=None):\n",
    "  \"\"\"Shared main loop for ResNet Models.\n",
    "  Args:\n",
    "    flags_obj: An object containing parsed flags. See define_resnet_flags()\n",
    "      for details.\n",
    "    model_function: the function that instantiates the Model and builds the\n",
    "      ops for train/eval. This will be passed directly into the estimator.\n",
    "    input_function: the function that processes the dataset and returns a\n",
    "      dataset that the estimator can train on. This will be wrapped with\n",
    "      all the relevant flags for running and passed to estimator.\n",
    "    dataset_name: the name of the dataset for training and evaluation. This is\n",
    "      used for logging purpose.\n",
    "    shape: list of ints representing the shape of the images used for training.\n",
    "      This is only used if flags_obj.export_dir is passed.\n",
    "  Returns:\n",
    "    Dict of results of the run.\n",
    "  \"\"\"\n",
    "\n",
    "  model_helpers.apply_clean(flags.FLAGS)\n",
    "\n",
    "  # Ensures flag override logic is only executed if explicitly triggered.\n",
    "  if flags_obj.tf_gpu_thread_mode:\n",
    "    override_flags_and_set_envars_for_gpu_thread_pool(flags_obj)\n",
    "\n",
    "  # Configures cluster spec for distribution strategy.\n",
    "  num_workers = distribution_utils.configure_cluster(flags_obj.worker_hosts,\n",
    "                                                     flags_obj.task_index)\n",
    "\n",
    "  # Creates session config. allow_soft_placement = True, is required for\n",
    "  # multi-GPU and is not harmful for other modes.\n",
    "  session_config = tf.compat.v1.ConfigProto(\n",
    "      inter_op_parallelism_threads=flags_obj.inter_op_parallelism_threads,\n",
    "      intra_op_parallelism_threads=flags_obj.intra_op_parallelism_threads,\n",
    "      allow_soft_placement=True)\n",
    "\n",
    "  distribution_strategy = distribution_utils.get_distribution_strategy(\n",
    "      distribution_strategy=flags_obj.distribution_strategy,\n",
    "      num_gpus=flags_core.get_num_gpus(flags_obj),\n",
    "      num_workers=num_workers,\n",
    "      all_reduce_alg=flags_obj.all_reduce_alg)\n",
    "\n",
    "  # Creates a `RunConfig` that checkpoints every 24 hours which essentially\n",
    "  # results in checkpoints determined only by `epochs_between_evals`.\n",
    "  run_config = tf.estimator.RunConfig(\n",
    "      train_distribute=distribution_strategy,\n",
    "      session_config=session_config,\n",
    "      save_checkpoints_secs=60*60*24)\n",
    "\n",
    "  # Initializes model with all but the dense layer from pretrained ResNet.\n",
    "  if flags_obj.pretrained_model_checkpoint_path is not None:\n",
    "    warm_start_settings = tf.estimator.WarmStartSettings(\n",
    "        flags_obj.pretrained_model_checkpoint_path,\n",
    "        vars_to_warm_start='^(?!.*dense)')\n",
    "  else:\n",
    "    warm_start_settings = None\n",
    "\n",
    "  classifier = tf.estimator.Estimator(\n",
    "      model_fn=model_function, model_dir=flags_obj.model_dir, config=run_config,\n",
    "      warm_start_from=warm_start_settings, params={\n",
    "          'resnet_size': int(flags_obj.resnet_size),\n",
    "          'data_format': flags_obj.data_format,\n",
    "          'batch_size': flags_obj.batch_size,\n",
    "          'resnet_version': int(flags_obj.resnet_version),\n",
    "          'loss_scale': flags_core.get_loss_scale(flags_obj),\n",
    "          'dtype': flags_core.get_tf_dtype(flags_obj),\n",
    "          'fine_tune': flags_obj.fine_tune\n",
    "      })\n",
    "\n",
    "  run_params = {\n",
    "      'batch_size': flags_obj.batch_size,\n",
    "      'dtype': flags_core.get_tf_dtype(flags_obj),\n",
    "      'resnet_size': flags_obj.resnet_size,\n",
    "      'resnet_version': flags_obj.resnet_version,\n",
    "      'synthetic_data': flags_obj.use_synthetic_data,\n",
    "      'train_epochs': flags_obj.train_epochs,\n",
    "  }\n",
    "  if flags_obj.use_synthetic_data:\n",
    "    dataset_name = dataset_name + '-synthetic'\n",
    "\n",
    "  benchmark_logger = logger.get_benchmark_logger()\n",
    "  benchmark_logger.log_run_info('resnet', dataset_name, run_params,\n",
    "                                test_id=flags_obj.benchmark_test_id)\n",
    "\n",
    "  train_hooks = hooks_helper.get_train_hooks(\n",
    "      flags_obj.hooks,\n",
    "      model_dir=flags_obj.model_dir,\n",
    "      batch_size=flags_obj.batch_size)\n",
    "\n",
    "  def input_fn_train(num_epochs):\n",
    "    return input_function(\n",
    "        is_training=True,\n",
    "        data_dir=flags_obj.data_dir,\n",
    "        batch_size=distribution_utils.per_device_batch_size(\n",
    "            flags_obj.batch_size, flags_core.get_num_gpus(flags_obj)),\n",
    "        num_epochs=num_epochs,\n",
    "        dtype=flags_core.get_tf_dtype(flags_obj),\n",
    "        datasets_num_private_threads=flags_obj.datasets_num_private_threads,\n",
    "        num_parallel_batches=flags_obj.datasets_num_parallel_batches)\n",
    "\n",
    "  def input_fn_eval():\n",
    "    return input_function(\n",
    "        is_training=False,\n",
    "        data_dir=flags_obj.data_dir,\n",
    "        batch_size=distribution_utils.per_device_batch_size(\n",
    "            flags_obj.batch_size, flags_core.get_num_gpus(flags_obj)),\n",
    "        num_epochs=1,\n",
    "        dtype=flags_core.get_tf_dtype(flags_obj))\n",
    "\n",
    "  train_epochs = (0 if flags_obj.eval_only or not flags_obj.train_epochs else\n",
    "                  flags_obj.train_epochs)\n",
    "\n",
    "  use_train_and_evaluate = flags_obj.use_train_and_evaluate or (\n",
    "      distribution_strategy.__class__.__name__ == 'CollectiveAllReduceStrategy')\n",
    "  if use_train_and_evaluate:\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=lambda: input_fn_train(train_epochs), hooks=train_hooks,\n",
    "        max_steps=flags_obj.max_train_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=input_fn_eval,\n",
    "                                      steps=flags_obj.max_train_steps)\n",
    "    tf.compat.v1.logging.info('Starting to train and evaluate.')\n",
    "    eval_results, _ = tf.estimator.train_and_evaluate(classifier, train_spec,\n",
    "                                                      eval_spec)\n",
    "    benchmark_logger.log_evaluation_result(eval_results)\n",
    "  else:\n",
    "    if train_epochs == 0:\n",
    "      # If --eval_only is set, perform a single loop with zero train epochs.\n",
    "      schedule, n_loops = [0], 1\n",
    "    else:\n",
    "      # Compute the number of times to loop while training. All but the last\n",
    "      # pass will train for `epochs_between_evals` epochs, while the last will\n",
    "      # train for the number needed to reach `training_epochs`. For instance if\n",
    "      #   train_epochs = 25 and epochs_between_evals = 10\n",
    "      # schedule will be set to [10, 10, 5]. That is to say, the loop will:\n",
    "      #   Train for 10 epochs and then evaluate.\n",
    "      #   Train for another 10 epochs and then evaluate.\n",
    "      #   Train for a final 5 epochs (to reach 25 epochs) and then evaluate.\n",
    "      n_loops = math.ceil(train_epochs / flags_obj.epochs_between_evals)\n",
    "      schedule = [flags_obj.epochs_between_evals for _ in range(int(n_loops))]\n",
    "      schedule[-1] = train_epochs - sum(schedule[:-1])  # over counting.\n",
    "\n",
    "    for cycle_index, num_train_epochs in enumerate(schedule):\n",
    "      tf.compat.v1.logging.info('Starting cycle: %d/%d', cycle_index,\n",
    "                                int(n_loops))\n",
    "\n",
    "      if num_train_epochs:\n",
    "        # Since we are calling classifier.train immediately in each loop, the\n",
    "        # value of num_train_epochs in the lambda function will not be changed\n",
    "        # before it is used. So it is safe to ignore the pylint error here\n",
    "        # pylint: disable=cell-var-from-loop\n",
    "        classifier.train(input_fn=lambda: input_fn_train(num_train_epochs),\n",
    "                         hooks=train_hooks, max_steps=flags_obj.max_train_steps)\n",
    "\n",
    "      # flags_obj.max_train_steps is generally associated with testing and\n",
    "      # profiling. As a result it is frequently called with synthetic data,\n",
    "      # which will iterate forever. Passing steps=flags_obj.max_train_steps\n",
    "      # allows the eval (which is generally unimportant in those circumstances)\n",
    "      # to terminate.  Note that eval will run for max_train_steps each loop,\n",
    "      # regardless of the global_step count.\n",
    "      tf.compat.v1.logging.info('Starting to evaluate.')\n",
    "      eval_results = classifier.evaluate(input_fn=input_fn_eval,\n",
    "                                         steps=flags_obj.max_train_steps)\n",
    "\n",
    "      benchmark_logger.log_evaluation_result(eval_results)\n",
    "\n",
    "      if model_helpers.past_stop_threshold(\n",
    "          flags_obj.stop_threshold, eval_results['accuracy']):\n",
    "        break\n",
    "\n",
    "  if flags_obj.export_dir is not None:\n",
    "    # Exports a saved model for the given classifier.\n",
    "    export_dtype = flags_core.get_tf_dtype(flags_obj)\n",
    "    if flags_obj.image_bytes_as_serving_input:\n",
    "      input_receiver_fn = functools.partial(\n",
    "          image_bytes_serving_input_fn, shape, dtype=export_dtype)\n",
    "    else:\n",
    "      input_receiver_fn = export.build_tensor_serving_input_receiver_fn(\n",
    "          shape, batch_size=flags_obj.batch_size, dtype=export_dtype)\n",
    "    classifier.export_savedmodel(flags_obj.export_dir, input_receiver_fn,\n",
    "                                 strip_default_attrs=True)\n",
    "\n",
    "  stats = {}\n",
    "  stats['eval_results'] = eval_results\n",
    "  stats['train_hooks'] = train_hooks\n",
    "\n",
    "  return stats\n",
    "\n",
    "\n",
    "def define_resnet_flags(resnet_size_choices=None):\n",
    "  \"\"\"Add flags and validators for ResNet.\"\"\"\n",
    "  flags_core.define_base()\n",
    "  flags_core.define_performance(num_parallel_calls=False,\n",
    "                                tf_gpu_thread_mode=True,\n",
    "                                datasets_num_private_threads=True,\n",
    "                                datasets_num_parallel_batches=True)\n",
    "  flags_core.define_image()\n",
    "  flags_core.define_benchmark()\n",
    "  flags.adopt_module_key_flags(flags_core)\n",
    "\n",
    "  flags.DEFINE_enum(\n",
    "      name='resnet_version', short_name='rv', default='1',\n",
    "      enum_values=['1', '2'],\n",
    "      help=flags_core.help_wrap(\n",
    "          'Version of ResNet. (1 or 2) See README.md for details.'))\n",
    "  flags.DEFINE_bool(\n",
    "      name='fine_tune', short_name='ft', default=False,\n",
    "      help=flags_core.help_wrap(\n",
    "          'If True do not train any parameters except for the final layer.'))\n",
    "  flags.DEFINE_string(\n",
    "      name='pretrained_model_checkpoint_path', short_name='pmcp', default=None,\n",
    "      help=flags_core.help_wrap(\n",
    "          'If not None initialize all the network except the final layer with '\n",
    "          'these values'))\n",
    "  flags.DEFINE_boolean(\n",
    "      name='eval_only', default=False,\n",
    "      help=flags_core.help_wrap('Skip training and only perform evaluation on '\n",
    "                                'the latest checkpoint.'))\n",
    "  flags.DEFINE_boolean(\n",
    "      name='image_bytes_as_serving_input', default=False,\n",
    "      help=flags_core.help_wrap(\n",
    "          'If True exports savedmodel with serving signature that accepts '\n",
    "          'JPEG image bytes instead of a fixed size [HxWxC] tensor that '\n",
    "          'represents the image. The former is easier to use for serving at '\n",
    "          'the expense of image resize/cropping being done as part of model '\n",
    "          'inference. Note, this flag only applies to ImageNet and cannot '\n",
    "          'be used for CIFAR.'))\n",
    "  flags.DEFINE_boolean(\n",
    "      name='use_train_and_evaluate', default=False,\n",
    "      help=flags_core.help_wrap(\n",
    "          'If True, uses `tf.estimator.train_and_evaluate` for the training '\n",
    "          'and evaluation loop, instead of separate calls to `classifier.train '\n",
    "          'and `classifier.evaluate`, which is the default behavior.'))\n",
    "  flags.DEFINE_string(\n",
    "      name='worker_hosts', default=None,\n",
    "      help=flags_core.help_wrap(\n",
    "          'Comma-separated list of worker ip:port pairs for running '\n",
    "          'multi-worker models with DistributionStrategy.  The user would '\n",
    "          'start the program on each host with identical value for this flag.'))\n",
    "  flags.DEFINE_integer(\n",
    "      name='task_index', default=-1,\n",
    "      help=flags_core.help_wrap('If multi-worker training, the task_index of '\n",
    "                                'this worker.'))\n",
    "  choice_kwargs = dict(\n",
    "      name='resnet_size', short_name='rs', default='50',\n",
    "      help=flags_core.help_wrap('The size of the ResNet model to use.'))\n",
    "\n",
    "  if resnet_size_choices is None:\n",
    "    flags.DEFINE_string(**choice_kwargs)\n",
    "  else:\n",
    "    flags.DEFINE_enum(enum_values=resnet_size_choices, **choice_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNlab",
   "language": "python",
   "name": "nnlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
