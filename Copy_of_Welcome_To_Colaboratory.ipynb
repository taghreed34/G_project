{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\">\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "metadata": {
        "id": "09_YG1rFw2mP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "xitplqMNk_Hc",
        "outputId": "ed4f60d2-878d-4056-c438-352dac39a112",
        "colab": {
          "height": 420
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Introducing Colaboratory\n",
        "#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('inN8seMm7UI', width=600, height=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=400\"\n",
              "            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GJBs_flRovLc"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "5626194c-e802-4293-942d-2908885c3c1f",
        "colab": {
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2fhs6GZ4qFMx"
      },
      "cell_type": "markdown",
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the ▷ button to the left of the code, or use the keyboard shortcut \"⌘/Ctrl+Enter\".\n",
        "\n",
        "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-gE-Ez1qtyIA",
        "outputId": "8d2e4259-4682-4e19-b683-7b9087f28820",
        "colab": {
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lSrWNr3MuFUS"
      },
      "cell_type": "markdown",
      "source": [
        "For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-Rh3-Vt9Nev9"
      },
      "cell_type": "markdown",
      "source": [
        "## More Resources\n",
        "\n",
        "Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n",
        "\n",
        "### Working with Notebooks in Colaboratory\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
        "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
        "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
        "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
        "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
        "\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "P-H6Lw1vyNNd"
      },
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Examples: Seedbank\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
        "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
        "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
        "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
        "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
      ]
    },
    {
      "metadata": {
        "id": "RanyVyvN-QqP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "_BATCH_NORM_DECAY = 0.997\n",
        "_BATCH_NORM_EPSILON = 1e-5\n",
        "DEFAULT_VERSION = 2\n",
        "DEFAULT_DTYPE = tf.float32\n",
        "CASTABLE_TYPES = (tf.float16,)\n",
        "ALLOWED_TYPES = (DEFAULT_DTYPE,) + CASTABLE_TYPES\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Convenience functions for building the ResNet model.\n",
        "################################################################################\n",
        "def batch_norm(inputs, training, data_format):\n",
        "  \"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n",
        "  # We set fused=True for a significant performance boost. See\n",
        "  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n",
        "  return tf.compat.v1.layers.batch_normalization(\n",
        "      inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n",
        "      momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n",
        "      scale=True, training=training, fused=True)\n",
        "\n",
        "\n",
        "def fixed_padding(inputs, kernel_size, data_format):\n",
        "  \"\"\"Pads the input along the spatial dimensions independently of input size.\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
        "      [batch, height_in, width_in, channels] depending on data_format.\n",
        "    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n",
        "                 Should be a positive integer.\n",
        "    data_format: The input format ('channels_last' or 'channels_first').\n",
        "  Returns:\n",
        "    A tensor with the same format as the input with the data either intact\n",
        "    (if kernel_size == 1) or padded (if kernel_size > 1).\n",
        "  \"\"\"\n",
        "  pad_total = kernel_size - 1\n",
        "  pad_beg = pad_total // 2\n",
        "  pad_end = pad_total - pad_beg\n",
        "\n",
        "  if data_format == 'channels_first':\n",
        "    padded_inputs = tf.pad(tensor=inputs,\n",
        "                           paddings=[[0, 0], [0, 0], [pad_beg, pad_end],\n",
        "                                     [pad_beg, pad_end]])\n",
        "  else:\n",
        "    padded_inputs = tf.pad(tensor=inputs,\n",
        "                           paddings=[[0, 0], [pad_beg, pad_end],\n",
        "                                     [pad_beg, pad_end], [0, 0]])\n",
        "  return padded_inputs\n",
        "\n",
        "\n",
        "def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):\n",
        "  \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n",
        "  # The padding is consistent and is based only on `kernel_size`, not on the\n",
        "  # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n",
        "  if strides > 1:\n",
        "    inputs = fixed_padding(inputs, kernel_size, data_format)\n",
        "\n",
        "  return tf.compat.v1.layers.conv2d(\n",
        "      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n",
        "      padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,\n",
        "      kernel_initializer=tf.compat.v1.variance_scaling_initializer(),\n",
        "      data_format=data_format)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# ResNet block definitions.\n",
        "################################################################################\n",
        "def _building_block_v1(inputs, filters, training, projection_shortcut, strides,\n",
        "                       data_format):\n",
        "  \"\"\"A single block for ResNet v1, without a bottleneck.\n",
        "  Convolution then batch normalization then ReLU as described by:\n",
        "    Deep Residual Learning for Image Recognition\n",
        "    https://arxiv.org/pdf/1512.03385.pdf\n",
        "    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
        "      [batch, height_in, width_in, channels] depending on data_format.\n",
        "    filters: The number of filters for the convolutions.\n",
        "    training: A Boolean for whether the model is in training or inference\n",
        "      mode. Needed for batch normalization.\n",
        "    projection_shortcut: The function to use for projection shortcuts\n",
        "      (typically a 1x1 convolution when downsampling the input).\n",
        "    strides: The block's stride. If greater than 1, this block will ultimately\n",
        "      downsample the input.\n",
        "    data_format: The input format ('channels_last' or 'channels_first').\n",
        "  Returns:\n",
        "    The output tensor of the block; shape should match inputs.\n",
        "  \"\"\"\n",
        "  shortcut = inputs\n",
        "\n",
        "  if projection_shortcut is not None:\n",
        "    shortcut = projection_shortcut(inputs)\n",
        "    shortcut = batch_norm(inputs=shortcut, training=training,\n",
        "                          data_format=data_format)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs += shortcut\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  return inputs\n",
        "\n",
        "\n",
        "def _building_block_v2(inputs, filters, training, projection_shortcut, strides,\n",
        "                       data_format):\n",
        "  \"\"\"A single block for ResNet v2, without a bottleneck.\n",
        "  Batch normalization then ReLu then convolution as described by:\n",
        "    Identity Mappings in Deep Residual Networks\n",
        "    https://arxiv.org/pdf/1603.05027.pdf\n",
        "    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
        "      [batch, height_in, width_in, channels] depending on data_format.\n",
        "    filters: The number of filters for the convolutions.\n",
        "    training: A Boolean for whether the model is in training or inference\n",
        "      mode. Needed for batch normalization.\n",
        "    projection_shortcut: The function to use for projection shortcuts\n",
        "      (typically a 1x1 convolution when downsampling the input).\n",
        "    strides: The block's stride. If greater than 1, this block will ultimately\n",
        "      downsample the input.\n",
        "    data_format: The input format ('channels_last' or 'channels_first').\n",
        "  Returns:\n",
        "    The output tensor of the block; shape should match inputs.\n",
        "  \"\"\"\n",
        "  shortcut = inputs\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  # The projection shortcut should come after the first batch norm and ReLU\n",
        "  # since it performs a 1x1 convolution.\n",
        "  if projection_shortcut is not None:\n",
        "    shortcut = projection_shortcut(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
        "      data_format=data_format)\n",
        "\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n",
        "      data_format=data_format)\n",
        "\n",
        "  return inputs + shortcut\n",
        "\n",
        "\n",
        "def _bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n",
        "                         strides, data_format):\n",
        "  \"\"\"A single block for ResNet v1, with a bottleneck.\n",
        "  Similar to _building_block_v1(), except using the \"bottleneck\" blocks\n",
        "  described in:\n",
        "    Convolution then batch normalization then ReLU as described by:\n",
        "      Deep Residual Learning for Image Recognition\n",
        "      https://arxiv.org/pdf/1512.03385.pdf\n",
        "      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
        "      [batch, height_in, width_in, channels] depending on data_format.\n",
        "    filters: The number of filters for the convolutions.\n",
        "    training: A Boolean for whether the model is in training or inference\n",
        "      mode. Needed for batch normalization.\n",
        "    projection_shortcut: The function to use for projection shortcuts\n",
        "      (typically a 1x1 convolution when downsampling the input).\n",
        "    strides: The block's stride. If greater than 1, this block will ultimately\n",
        "      downsample the input.\n",
        "    data_format: The input format ('channels_last' or 'channels_first').\n",
        "  Returns:\n",
        "    The output tensor of the block; shape should match inputs.\n",
        "  \"\"\"\n",
        "  shortcut = inputs\n",
        "\n",
        "  if projection_shortcut is not None:\n",
        "    shortcut = projection_shortcut(inputs)\n",
        "    shortcut = batch_norm(inputs=shortcut, training=training,\n",
        "                          data_format=data_format)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n",
        "      data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs += shortcut\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  return inputs\n",
        "\n",
        "\n",
        "def _bottleneck_block_v2(inputs, filters, training, projection_shortcut,\n",
        "                         strides, data_format):\n",
        "  \"\"\"A single block for ResNet v2, with a bottleneck.\n",
        "  Similar to _building_block_v2(), except using the \"bottleneck\" blocks\n",
        "  described in:\n",
        "    Convolution then batch normalization then ReLU as described by:\n",
        "      Deep Residual Learning for Image Recognition\n",
        "      https://arxiv.org/pdf/1512.03385.pdf\n",
        "      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n",
        "  Adapted to the ordering conventions of:\n",
        "    Batch normalization then ReLu then convolution as described by:\n",
        "      Identity Mappings in Deep Residual Networks\n",
        "      https://arxiv.org/pdf/1603.05027.pdf\n",
        "      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
        "      [batch, height_in, width_in, channels] depending on data_format.\n",
        "    filters: The number of filters for the convolutions.\n",
        "    training: A Boolean for whether the model is in training or inference\n",
        "      mode. Needed for batch normalization.\n",
        "    projection_shortcut: The function to use for projection shortcuts\n",
        "      (typically a 1x1 convolution when downsampling the input).\n",
        "    strides: The block's stride. If greater than 1, this block will ultimately\n",
        "      downsample the input.\n",
        "    data_format: The input format ('channels_last' or 'channels_first').\n",
        "  Returns:\n",
        "    The output tensor of the block; shape should match inputs.\n",
        "  \"\"\"\n",
        "  shortcut = inputs\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "\n",
        "  # The projection shortcut should come after the first batch norm and ReLU\n",
        "  # since it performs a 1x1 convolution.\n",
        "  if projection_shortcut is not None:\n",
        "    shortcut = projection_shortcut(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n",
        "      data_format=data_format)\n",
        "\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
        "      data_format=data_format)\n",
        "\n",
        "  inputs = batch_norm(inputs, training, data_format)\n",
        "  inputs = tf.nn.relu(inputs)\n",
        "  inputs = conv2d_fixed_padding(\n",
        "      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n",
        "      data_format=data_format)\n",
        "\n",
        "  return inputs + shortcut\n",
        "\n",
        "\n",
        "def block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,\n",
        "                training, name, data_format):\n",
        "  \"\"\"Creates one layer of blocks for the ResNet model.\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
        "      [batch, height_in, width_in, channels] depending on data_format.\n",
        "    filters: The number of filters for the first convolution of the layer.\n",
        "    bottleneck: Is the block created a bottleneck block.\n",
        "    block_fn: The block to use within the model, either `building_block` or\n",
        "      `bottleneck_block`.\n",
        "    blocks: The number of blocks contained in the layer.\n",
        "    strides: The stride to use for the first convolution of the layer. If\n",
        "      greater than 1, this layer will ultimately downsample the input.\n",
        "    training: Either True or False, whether we are currently training the\n",
        "      model. Needed for batch norm.\n",
        "    name: A string name for the tensor output of the block layer.\n",
        "    data_format: The input format ('channels_last' or 'channels_first').\n",
        "  Returns:\n",
        "    The output tensor of the block layer.\n",
        "  \"\"\"\n",
        "\n",
        "  # Bottleneck blocks end with 4x the number of filters as they start with\n",
        "  filters_out = filters * 4 if bottleneck else filters\n",
        "\n",
        "  def projection_shortcut(inputs):\n",
        "    return conv2d_fixed_padding(\n",
        "        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n",
        "        data_format=data_format)\n",
        "\n",
        "  # Only the first block per block_layer uses projection_shortcut and strides\n",
        "  inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n",
        "                    data_format)\n",
        "\n",
        "  for _ in range(1, blocks):\n",
        "    inputs = block_fn(inputs, filters, training, None, 1, data_format)\n",
        "\n",
        "  return tf.identity(inputs, name)\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "  \"\"\"Base class for building the Resnet Model.\"\"\"\n",
        "\n",
        "  def __init__(self, resnet_size, bottleneck, num_classes, num_filters,\n",
        "               kernel_size,\n",
        "               conv_stride, first_pool_size, first_pool_stride,\n",
        "               block_sizes, block_strides,\n",
        "               resnet_version=DEFAULT_VERSION, data_format=None,\n",
        "               dtype=DEFAULT_DTYPE):\n",
        "    \"\"\"Creates a model for classifying an image.\n",
        "    Args:\n",
        "      resnet_size: A single integer for the size of the ResNet model.\n",
        "      bottleneck: Use regular blocks or bottleneck blocks.\n",
        "      num_classes: The number of classes used as labels.\n",
        "      num_filters: The number of filters to use for the first block layer\n",
        "        of the model. This number is then doubled for each subsequent block\n",
        "        layer.\n",
        "      kernel_size: The kernel size to use for convolution.\n",
        "      conv_stride: stride size for the initial convolutional layer\n",
        "      first_pool_size: Pool size to be used for the first pooling layer.\n",
        "        If none, the first pooling layer is skipped.\n",
        "      first_pool_stride: stride size for the first pooling layer. Not used\n",
        "        if first_pool_size is None.\n",
        "      block_sizes: A list containing n values, where n is the number of sets of\n",
        "        block layers desired. Each value should be the number of blocks in the\n",
        "        i-th set.\n",
        "      block_strides: List of integers representing the desired stride size for\n",
        "        each of the sets of block layers. Should be same length as block_sizes.\n",
        "      resnet_version: Integer representing which version of the ResNet network\n",
        "        to use. See README for details. Valid values: [1, 2]\n",
        "      data_format: Input format ('channels_last', 'channels_first', or None).\n",
        "        If set to None, the format is dependent on whether a GPU is available.\n",
        "      dtype: The TensorFlow dtype to use for calculations. If not specified\n",
        "        tf.float32 is used.\n",
        "    Raises:\n",
        "      ValueError: if invalid version is selected.\n",
        "    \"\"\"\n",
        "    self.resnet_size = resnet_size\n",
        "\n",
        "    if not data_format:\n",
        "      data_format = (\n",
        "          'channels_first' if tf.test.is_built_with_cuda() else 'channels_last')\n",
        "\n",
        "    self.resnet_version = resnet_version\n",
        "    if resnet_version not in (1, 2):\n",
        "      raise ValueError(\n",
        "          'Resnet version should be 1 or 2. See README for citations.')\n",
        "\n",
        "    self.bottleneck = bottleneck\n",
        "    if bottleneck:\n",
        "      if resnet_version == 1:\n",
        "        self.block_fn = _bottleneck_block_v1\n",
        "      else:\n",
        "        self.block_fn = _bottleneck_block_v2\n",
        "    else:\n",
        "      if resnet_version == 1:\n",
        "        self.block_fn = _building_block_v1\n",
        "      else:\n",
        "        self.block_fn = _building_block_v2\n",
        "\n",
        "    if dtype not in ALLOWED_TYPES:\n",
        "      raise ValueError('dtype must be one of: {}'.format(ALLOWED_TYPES))\n",
        "\n",
        "    self.data_format = data_format\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "    self.kernel_size = kernel_size\n",
        "    self.conv_stride = conv_stride\n",
        "    self.first_pool_size = first_pool_size\n",
        "    self.first_pool_stride = first_pool_stride\n",
        "    self.block_sizes = block_sizes\n",
        "    self.block_strides = block_strides\n",
        "    self.dtype = dtype\n",
        "    self.pre_activation = resnet_version == 2\n",
        "\n",
        "  def _custom_dtype_getter(self, getter, name, shape=None, dtype=DEFAULT_DTYPE,\n",
        "                           *args, **kwargs):\n",
        "    \"\"\"Creates variables in fp32, then casts to fp16 if necessary.\n",
        "    This function is a custom getter. A custom getter is a function with the\n",
        "    same signature as tf.get_variable, except it has an additional getter\n",
        "    parameter. Custom getters can be passed as the `custom_getter` parameter of\n",
        "    tf.variable_scope. Then, tf.get_variable will call the custom getter,\n",
        "    instead of directly getting a variable itself. This can be used to change\n",
        "    the types of variables that are retrieved with tf.get_variable.\n",
        "    The `getter` parameter is the underlying variable getter, that would have\n",
        "    been called if no custom getter was used. Custom getters typically get a\n",
        "    variable with `getter`, then modify it in some way.\n",
        "    This custom getter will create an fp32 variable. If a low precision\n",
        "    (e.g. float16) variable was requested it will then cast the variable to the\n",
        "    requested dtype. The reason we do not directly create variables in low\n",
        "    precision dtypes is that applying small gradients to such variables may\n",
        "    cause the variable not to change.\n",
        "    Args:\n",
        "      getter: The underlying variable getter, that has the same signature as\n",
        "        tf.get_variable and returns a variable.\n",
        "      name: The name of the variable to get.\n",
        "      shape: The shape of the variable to get.\n",
        "      dtype: The dtype of the variable to get. Note that if this is a low\n",
        "        precision dtype, the variable will be created as a tf.float32 variable,\n",
        "        then cast to the appropriate dtype\n",
        "      *args: Additional arguments to pass unmodified to getter.\n",
        "      **kwargs: Additional keyword arguments to pass unmodified to getter.\n",
        "    Returns:\n",
        "      A variable which is cast to fp16 if necessary.\n",
        "    \"\"\"\n",
        "\n",
        "    if dtype in CASTABLE_TYPES:\n",
        "      var = getter(name, shape, tf.float32, *args, **kwargs)\n",
        "      return tf.cast(var, dtype=dtype, name=name + '_cast')\n",
        "    else:\n",
        "      return getter(name, shape, dtype, *args, **kwargs)\n",
        "\n",
        "  def _model_variable_scope(self):\n",
        "    \"\"\"Returns a variable scope that the model should be created under.\n",
        "    If self.dtype is a castable type, model variable will be created in fp32\n",
        "    then cast to self.dtype before being used.\n",
        "    Returns:\n",
        "      A variable scope for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    return tf.compat.v1.variable_scope('resnet_model',\n",
        "                                       custom_getter=self._custom_dtype_getter)\n",
        "\n",
        "  def __call__(self, inputs, training):\n",
        "    \"\"\"Add operations to classify a batch of input images.\n",
        "    Args:\n",
        "      inputs: A Tensor representing a batch of input images.\n",
        "      training: A boolean. Set to True to add operations required only when\n",
        "        training the classifier.\n",
        "    Returns:\n",
        "      A logits Tensor with shape [<batch_size>, self.num_classes].\n",
        "    \"\"\"\n",
        "\n",
        "    with self._model_variable_scope():\n",
        "      if self.data_format == 'channels_first':\n",
        "        # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).\n",
        "        # This provides a large performance boost on GPU. See\n",
        "        # https://www.tensorflow.org/performance/performance_guide#data_formats\n",
        "        inputs = tf.transpose(a=inputs, perm=[0, 3, 1, 2])\n",
        "\n",
        "      inputs = conv2d_fixed_padding(\n",
        "          inputs=inputs, filters=self.num_filters, kernel_size=self.kernel_size,\n",
        "          strides=self.conv_stride, data_format=self.data_format)\n",
        "      inputs = tf.identity(inputs, 'initial_conv')\n",
        "\n",
        "      # We do not include batch normalization or activation functions in V2\n",
        "      # for the initial conv1 because the first ResNet unit will perform these\n",
        "      # for both the shortcut and non-shortcut paths as part of the first\n",
        "      # block's projection. Cf. Appendix of [2].\n",
        "      if self.resnet_version == 1:\n",
        "        inputs = batch_norm(inputs, training, self.data_format)\n",
        "        inputs = tf.nn.relu(inputs)\n",
        "\n",
        "      if self.first_pool_size:\n",
        "        inputs = tf.compat.v1.layers.max_pooling2d(\n",
        "            inputs=inputs, pool_size=self.first_pool_size,\n",
        "            strides=self.first_pool_stride, padding='SAME',\n",
        "            data_format=self.data_format)\n",
        "        inputs = tf.identity(inputs, 'initial_max_pool')\n",
        "\n",
        "      for i, num_blocks in enumerate(self.block_sizes):\n",
        "        num_filters = self.num_filters * (2**i)\n",
        "        inputs = block_layer(\n",
        "            inputs=inputs, filters=num_filters, bottleneck=self.bottleneck,\n",
        "            block_fn=self.block_fn, blocks=num_blocks,\n",
        "            strides=self.block_strides[i], training=training,\n",
        "            name='block_layer{}'.format(i + 1), data_format=self.data_format)\n",
        "\n",
        "      # Only apply the BN and ReLU for model that does pre_activation in each\n",
        "      # building/bottleneck block, eg resnet V2.\n",
        "      if self.pre_activation:\n",
        "        inputs = batch_norm(inputs, training, self.data_format)\n",
        "        inputs = tf.nn.relu(inputs)\n",
        "\n",
        "      # The current top layer has shape\n",
        "      # `batch_size x pool_size x pool_size x final_size`.\n",
        "      # ResNet does an Average Pooling layer over pool_size,\n",
        "      # but that is the same as doing a reduce_mean. We do a reduce_mean\n",
        "      # here because it performs better than AveragePooling2D.\n",
        "      axes = [2, 3] if self.data_format == 'channels_first' else [1, 2]\n",
        "      inputs = tf.reduce_mean(input_tensor=inputs, axis=axes, keepdims=True)\n",
        "      inputs = tf.identity(inputs, 'final_reduce_mean')\n",
        "\n",
        "      inputs = tf.squeeze(inputs, axes)\n",
        "      inputs = tf.compat.v1.layers.dense(inputs=inputs, units=self.num_classes)\n",
        "      inputs = tf.identity(inputs, 'final_dense')\n",
        "      return inputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dpDZsUoyYvAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import functools\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "# pylint: disable=g-bad-import-order\n",
        "from absl import flags\n",
        "import tensorflow as tf\n",
        "\n",
        "# Functions for input processing.\n",
        "################################################################################\n",
        "def process_record_dataset(dataset,\n",
        "                           is_training,\n",
        "                           batch_size,\n",
        "                           shuffle_buffer,\n",
        "                           parse_record_fn,\n",
        "                           num_epochs=1,\n",
        "                           dtype=tf.float32,\n",
        "                           datasets_num_private_threads=None,\n",
        "                           num_parallel_batches=1):\n",
        "  \"\"\"Given a Dataset with raw records, return an iterator over the records.\n",
        "  Args:\n",
        "    dataset: A Dataset representing raw records\n",
        "    is_training: A boolean denoting whether the input is for training.\n",
        "    batch_size: The number of samples per batch.\n",
        "    shuffle_buffer: The buffer size to use when shuffling records. A larger\n",
        "      value results in better randomness, but smaller values reduce startup\n",
        "      time and use less memory.\n",
        "    parse_record_fn: A function that takes a raw record and returns the\n",
        "      corresponding (image, label) pair.\n",
        "    num_epochs: The number of epochs to repeat the dataset.\n",
        "    dtype: Data type to use for images/features.\n",
        "    datasets_num_private_threads: Number of threads for a private\n",
        "      threadpool created for all datasets computation.\n",
        "    num_parallel_batches: Number of parallel batches for tf.data.\n",
        "  Returns:\n",
        "    Dataset of (image, label) pairs ready for iteration.\n",
        "  \"\"\"\n",
        "  # Defines a specific size thread pool for tf.data operations.\n",
        "  if datasets_num_private_threads:\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_threading.private_threadpool_size = (\n",
        "        datasets_num_private_threads)\n",
        "    dataset = dataset.with_options(options)\n",
        "    tf.compat.v1.logging.info('datasets_num_private_threads: %s',\n",
        "                              datasets_num_private_threads)\n",
        "\n",
        "  # Prefetches a batch at a time to smooth out the time taken to load input\n",
        "  # files for shuffling and processing.\n",
        "  dataset = dataset.prefetch(buffer_size=batch_size)\n",
        "  if is_training:\n",
        "    # Shuffles records before repeating to respect epoch boundaries.\n",
        "    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
        "\n",
        "  # Repeats the dataset for the number of epochs to train.\n",
        "  dataset = dataset.repeat(num_epochs)\n",
        "\n",
        "  # Parses the raw records into images and labels.\n",
        "  dataset = dataset.apply(\n",
        "      tf.data.experimental.map_and_batch(\n",
        "          lambda value: parse_record_fn(value, is_training, dtype),\n",
        "          batch_size=batch_size,\n",
        "          num_parallel_batches=num_parallel_batches,\n",
        "          drop_remainder=False))\n",
        "\n",
        "  # Operations between the final prefetch and the get_next call to the iterator\n",
        "  # will happen synchronously during run time. We prefetch here again to\n",
        "  # background all of the above processing work and keep it out of the\n",
        "  # critical training path. Setting buffer_size to tf.contrib.data.AUTOTUNE\n",
        "  # allows DistributionStrategies to adjust how many batches to fetch based\n",
        "  # on how many devices are present.\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def get_synth_input_fn(height, width, num_channels, num_classes,\n",
        "                       dtype=tf.float32):\n",
        "  \"\"\"Returns an input function that returns a dataset with random data.\n",
        "  This input_fn returns a data set that iterates over a set of random data and\n",
        "  bypasses all preprocessing, e.g. jpeg decode and copy. The host to device\n",
        "  copy is still included. This used to find the upper throughput bound when\n",
        "  tunning the full input pipeline.\n",
        "  Args:\n",
        "    height: Integer height that will be used to create a fake image tensor.\n",
        "    width: Integer width that will be used to create a fake image tensor.\n",
        "    num_channels: Integer depth that will be used to create a fake image tensor.\n",
        "    num_classes: Number of classes that should be represented in the fake labels\n",
        "      tensor\n",
        "    dtype: Data type for features/images.\n",
        "  Returns:\n",
        "    An input_fn that can be used in place of a real one to return a dataset\n",
        "    that can be used for iteration.\n",
        "  \"\"\"\n",
        "  # pylint: disable=unused-argument\n",
        "  def input_fn(is_training, data_dir, batch_size, *args, **kwargs):\n",
        "    \"\"\"Returns dataset filled with random data.\"\"\"\n",
        "    # Synthetic input should be within [0, 255].\n",
        "    inputs = tf.random.truncated_normal(\n",
        "        [batch_size] + [height, width, num_channels],\n",
        "        dtype=dtype,\n",
        "        mean=127,\n",
        "        stddev=60,\n",
        "        name='synthetic_inputs')\n",
        "\n",
        "    labels = tf.random.uniform(\n",
        "        [batch_size],\n",
        "        minval=0,\n",
        "        maxval=num_classes - 1,\n",
        "        dtype=tf.int32,\n",
        "        name='synthetic_labels')\n",
        "    data = tf.data.Dataset.from_tensors((inputs, labels)).repeat()\n",
        "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return data\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def image_bytes_serving_input_fn(image_shape, dtype=tf.float32):\n",
        "  \"\"\"Serving input fn for raw jpeg images.\"\"\"\n",
        "\n",
        "  def _preprocess_image(image_bytes):\n",
        "    \"\"\"Preprocess a single raw image.\"\"\"\n",
        "    # Bounding box around the whole image.\n",
        "    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=dtype, shape=[1, 1, 4])\n",
        "    height, width, num_channels = image_shape\n",
        "    image = imagenet_preprocessing.preprocess_image(\n",
        "        image_bytes, bbox, height, width, num_channels, is_training=False)\n",
        "    return image\n",
        "\n",
        "  image_bytes_list = tf.compat.v1.placeholder(\n",
        "      shape=[None], dtype=tf.string, name='input_tensor')\n",
        "  images = tf.map_fn(\n",
        "      _preprocess_image, image_bytes_list, back_prop=False, dtype=dtype)\n",
        "  return tf.estimator.export.TensorServingInputReceiver(\n",
        "      images, {'image_bytes': image_bytes_list})\n",
        "\n",
        "\n",
        "def override_flags_and_set_envars_for_gpu_thread_pool(flags_obj):\n",
        "  \"\"\"Override flags and set env_vars for performance.\n",
        "  These settings exist to test the difference between using stock settings\n",
        "  and manual tuning. It also shows some of the ENV_VARS that can be tweaked to\n",
        "  squeeze a few extra examples per second.  These settings are defaulted to the\n",
        "  current platform of interest, which changes over time.\n",
        "  On systems with small numbers of cpu cores, e.g. under 8 logical cores,\n",
        "  setting up a gpu thread pool with `tf_gpu_thread_mode=gpu_private` may perform\n",
        "  poorly.\n",
        "  Args:\n",
        "    flags_obj: Current flags, which will be adjusted possibly overriding\n",
        "    what has been set by the user on the command-line.\n",
        "  \"\"\"\n",
        "  cpu_count = multiprocessing.cpu_count()\n",
        "  tf.compat.v1.logging.info('Logical CPU cores: %s', cpu_count)\n",
        "\n",
        "  # Sets up thread pool for each GPU for op scheduling.\n",
        "  per_gpu_thread_count = 1\n",
        "  total_gpu_thread_count = per_gpu_thread_count * flags_obj.num_gpus\n",
        "  os.environ['TF_GPU_THREAD_MODE'] = flags_obj.tf_gpu_thread_mode\n",
        "  os.environ['TF_GPU_THREAD_COUNT'] = str(per_gpu_thread_count)\n",
        "  tf.compat.v1.logging.info('TF_GPU_THREAD_COUNT: %s',\n",
        "                            os.environ['TF_GPU_THREAD_COUNT'])\n",
        "  tf.compat.v1.logging.info('TF_GPU_THREAD_MODE: %s',\n",
        "                            os.environ['TF_GPU_THREAD_MODE'])\n",
        "\n",
        "  # Reduces general thread pool by number of threads used for GPU pool.\n",
        "  main_thread_count = cpu_count - total_gpu_thread_count\n",
        "  flags_obj.inter_op_parallelism_threads = main_thread_count\n",
        "\n",
        "  # Sets thread count for tf.data. Logical cores minus threads assign to the\n",
        "  # private GPU pool along with 2 thread per GPU for event monitoring and\n",
        "  # sending / receiving tensors.\n",
        "  num_monitoring_threads = 2 * flags_obj.num_gpus\n",
        "  flags_obj.datasets_num_private_threads = (cpu_count - total_gpu_thread_count\n",
        "                                            - num_monitoring_threads)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Functions for running training/eval/validation loops for the model.\n",
        "################################################################################\n",
        "def learning_rate_with_decay(\n",
        "    batch_size, batch_denom, num_images, boundary_epochs, decay_rates,\n",
        "    base_lr=0.1, warmup=False):\n",
        "  \"\"\"Get a learning rate that decays step-wise as training progresses.\n",
        "  Args:\n",
        "    batch_size: the number of examples processed in each training batch.\n",
        "    batch_denom: this value will be used to scale the base learning rate.\n",
        "      `0.1 * batch size` is divided by this number, such that when\n",
        "      batch_denom == batch_size, the initial learning rate will be 0.1.\n",
        "    num_images: total number of images that will be used for training.\n",
        "    boundary_epochs: list of ints representing the epochs at which we\n",
        "      decay the learning rate.\n",
        "    decay_rates: list of floats representing the decay rates to be used\n",
        "      for scaling the learning rate. It should have one more element\n",
        "      than `boundary_epochs`, and all elements should have the same type.\n",
        "    base_lr: Initial learning rate scaled based on batch_denom.\n",
        "    warmup: Run a 5 epoch warmup to the initial lr.\n",
        "  Returns:\n",
        "    Returns a function that takes a single argument - the number of batches\n",
        "    trained so far (global_step)- and returns the learning rate to be used\n",
        "    for training the next batch.\n",
        "  \"\"\"\n",
        "  initial_learning_rate = base_lr * batch_size / batch_denom\n",
        "  batches_per_epoch = num_images / batch_size\n",
        "\n",
        "  # Reduce the learning rate at certain epochs.\n",
        "  # CIFAR-10: divide by 10 at epoch 100, 150, and 200\n",
        "  # ImageNet: divide by 10 at epoch 30, 60, 80, and 90\n",
        "  boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n",
        "  vals = [initial_learning_rate * decay for decay in decay_rates]\n",
        "\n",
        "  def learning_rate_fn(global_step):\n",
        "    \"\"\"Builds scaled learning rate function with 5 epoch warm up.\"\"\"\n",
        "    lr = tf.compat.v1.train.piecewise_constant(global_step, boundaries, vals)\n",
        "    if warmup:\n",
        "      warmup_steps = int(batches_per_epoch * 5)\n",
        "      warmup_lr = (\n",
        "          initial_learning_rate * tf.cast(global_step, tf.float32) / tf.cast(\n",
        "              warmup_steps, tf.float32))\n",
        "      return tf.cond(pred=global_step < warmup_steps,\n",
        "                     true_fn=lambda: warmup_lr,\n",
        "                     false_fn=lambda: lr)\n",
        "    return lr\n",
        "\n",
        "  return learning_rate_fn\n",
        "\n",
        "\n",
        "def resnet_model_fn(features, labels, mode, model_class,\n",
        "                    resnet_size, weight_decay, learning_rate_fn, momentum,\n",
        "                    data_format, resnet_version, loss_scale,\n",
        "                    loss_filter_fn=None,\n",
        "                    fine_tune=False):\n",
        "  \"\"\"Shared functionality for different resnet model_fns.\n",
        "  Initializes the ResnetModel representing the model layers\n",
        "  and uses that model to build the necessary EstimatorSpecs for\n",
        "  the `mode` in question. For training, this means building losses,\n",
        "  the optimizer, and the train op that get passed into the EstimatorSpec.\n",
        "  For evaluation and prediction, the EstimatorSpec is returned without\n",
        "  a train op, but with the necessary parameters for the given mode.\n",
        "  Args:\n",
        "    features: tensor representing input images\n",
        "    labels: tensor representing class labels for all input images\n",
        "    mode: current estimator mode; should be one of\n",
        "      `tf.estimator.ModeKeys.TRAIN`, `EVALUATE`, `PREDICT`\n",
        "    model_class: a class representing a TensorFlow model that has a __call__\n",
        "      function. We assume here that this is a subclass of ResnetModel.\n",
        "    resnet_size: A single integer for the size of the ResNet model.\n",
        "    weight_decay: weight decay loss rate used to regularize learned variables.\n",
        "    learning_rate_fn: function that returns the current learning rate given\n",
        "      the current global_step\n",
        "    momentum: momentum term used for optimization\n",
        "    data_format: Input format ('channels_last', 'channels_first', or None).\n",
        "      If set to None, the format is dependent on whether a GPU is available.\n",
        "    resnet_version: Integer representing which version of the ResNet network to\n",
        "      use. See README for details. Valid values: [1, 2]\n",
        "    loss_scale: The factor to scale the loss for numerical stability. A detailed\n",
        "      summary is present in the arg parser help text.\n",
        "    loss_filter_fn: function that takes a string variable name and returns\n",
        "      True if the var should be included in loss calculation, and False\n",
        "      otherwise. If None, batch_normalization variables will be excluded\n",
        "      from the loss.\n",
        "    dtype: the TensorFlow dtype to use for calculations.\n",
        "    fine_tune: If True only train the dense layers(final layers).\n",
        "  Returns:\n",
        "    EstimatorSpec parameterized according to the input params and the\n",
        "    current mode.\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate a summary node for the images\n",
        "  tf.compat.v1.summary.image('images', features, max_outputs=6)\n",
        "  # Checks that features/images have same data type being used for calculations.\n",
        "  assert features.dtype == dtype\n",
        "\n",
        "  model = model_class(resnet_size, data_format, resnet_version=resnet_version,\n",
        "                      dtype=dtype)\n",
        "\n",
        "  logits = model(features, mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "  # This acts as a no-op if the logits are already in fp32 (provided logits are\n",
        "  # not a SparseTensor). If dtype is is low precision, logits must be cast to\n",
        "  # fp32 for numerical stability.\n",
        "  logits = tf.cast(logits, tf.float32)\n",
        "\n",
        "  predictions = {\n",
        "      'classes': tf.argmax(input=logits, axis=1),\n",
        "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
        "  }\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    # Return the predictions and the specification for serving a SavedModel\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions=predictions,\n",
        "        export_outputs={\n",
        "            'predict': tf.estimator.export.PredictOutput(predictions)\n",
        "        })\n",
        "\n",
        "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
        "  cross_entropy = tf.compat.v1.losses.sparse_softmax_cross_entropy(\n",
        "      logits=logits, labels=labels)\n",
        "\n",
        "  # Create a tensor named cross_entropy for logging purposes.\n",
        "  tf.identity(cross_entropy, name='cross_entropy')\n",
        "  tf.compat.v1.summary.scalar('cross_entropy', cross_entropy)\n",
        "\n",
        "  # If no loss_filter_fn is passed, assume we want the default behavior,\n",
        "  # which is that batch_normalization variables are excluded from loss.\n",
        "  def exclude_batch_norm(name):\n",
        "    return 'batch_normalization' not in name\n",
        "  loss_filter_fn = loss_filter_fn or exclude_batch_norm\n",
        "\n",
        "  # Add weight decay to the loss. We need to scale the regularization loss\n",
        "  # manually as losses other than in tf.losses and tf.keras.losses don't scale\n",
        "  # automatically.\n",
        "  l2_loss = weight_decay * tf.add_n(\n",
        "      # loss is computed using fp32 for numerical stability.\n",
        "      [\n",
        "          tf.nn.l2_loss(tf.cast(v, tf.float32))\n",
        "          for v in tf.compat.v1.trainable_variables()\n",
        "          if loss_filter_fn(v.name)\n",
        "      ]) / tf.distribute.get_strategy().num_replicas_in_sync\n",
        "  tf.compat.v1.summary.scalar('l2_loss', l2_loss)\n",
        "  loss = cross_entropy + l2_loss\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "    learning_rate = learning_rate_fn(global_step)\n",
        "\n",
        "    # Create a tensor named learning_rate for logging purposes\n",
        "    tf.identity(learning_rate, name='learning_rate')\n",
        "    tf.compat.v1.summary.scalar('learning_rate', learning_rate)\n",
        "\n",
        "    optimizer = tf.compat.v1.train.MomentumOptimizer(\n",
        "        learning_rate=learning_rate,\n",
        "        momentum=momentum\n",
        "    )\n",
        "\n",
        "    def _dense_grad_filter(gvs):\n",
        "      \"\"\"Only apply gradient updates to the final layer.\n",
        "      This function is used for fine tuning.\n",
        "      Args:\n",
        "        gvs: list of tuples with gradients and variable info\n",
        "      Returns:\n",
        "        filtered gradients so that only the dense layer remains\n",
        "      \"\"\"\n",
        "      return [(g, v) for g, v in gvs if 'dense' in v.name]\n",
        "\n",
        "    if loss_scale != 1:\n",
        "      # When computing fp16 gradients, often intermediate tensor values are\n",
        "      # so small, they underflow to 0. To avoid this, we multiply the loss by\n",
        "      # loss_scale to make these tensor values loss_scale times bigger.\n",
        "      scaled_grad_vars = optimizer.compute_gradients(loss * loss_scale)\n",
        "\n",
        "      if fine_tune:\n",
        "        scaled_grad_vars = _dense_grad_filter(scaled_grad_vars)\n",
        "\n",
        "      # Once the gradient computation is complete we can scale the gradients\n",
        "      # back to the correct scale before passing them to the optimizer.\n",
        "      unscaled_grad_vars = [(grad / loss_scale, var)\n",
        "                            for grad, var in scaled_grad_vars]\n",
        "      minimize_op = optimizer.apply_gradients(unscaled_grad_vars, global_step)\n",
        "    else:\n",
        "      grad_vars = optimizer.compute_gradients(loss)\n",
        "      if fine_tune:\n",
        "        grad_vars = _dense_grad_filter(grad_vars)\n",
        "      minimize_op = optimizer.apply_gradients(grad_vars, global_step)\n",
        "\n",
        "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
        "    train_op = tf.group(minimize_op, update_ops)\n",
        "  else:\n",
        "    train_op = None\n",
        "\n",
        "  accuracy = tf.compat.v1.metrics.accuracy(labels, predictions['classes'])\n",
        "  accuracy_top_5 = tf.compat.v1.metrics.mean(\n",
        "      tf.nn.in_top_k(predictions=logits, targets=labels, k=5, name='top_5_op'))\n",
        "  metrics = {'accuracy': accuracy,\n",
        "             'accuracy_top_5': accuracy_top_5}\n",
        "\n",
        "  # Create a tensor named train_accuracy for logging purposes\n",
        "  tf.identity(accuracy[1], name='train_accuracy')\n",
        "  tf.identity(accuracy_top_5[1], name='train_accuracy_top_5')\n",
        "  tf.compat.v1.summary.scalar('train_accuracy', accuracy[1])\n",
        "  tf.compat.v1.summary.scalar('train_accuracy_top_5', accuracy_top_5[1])\n",
        "\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "      predictions=predictions,\n",
        "      loss=loss,\n",
        "      train_op=train_op,\n",
        "      eval_metric_ops=metrics)\n",
        "\n",
        "\n",
        "def resnet_main(\n",
        "    flags_obj, model_function, input_function, dataset_name, shape=None):\n",
        "  \"\"\"Shared main loop for ResNet Models.\n",
        "  Args:\n",
        "    flags_obj: An object containing parsed flags. See define_resnet_flags()\n",
        "      for details.\n",
        "    model_function: the function that instantiates the Model and builds the\n",
        "      ops for train/eval. This will be passed directly into the estimator.\n",
        "    input_function: the function that processes the dataset and returns a\n",
        "      dataset that the estimator can train on. This will be wrapped with\n",
        "      all the relevant flags for running and passed to estimator.\n",
        "    dataset_name: the name of the dataset for training and evaluation. This is\n",
        "      used for logging purpose.\n",
        "    shape: list of ints representing the shape of the images used for training.\n",
        "      This is only used if flags_obj.export_dir is passed.\n",
        "  Returns:\n",
        "    Dict of results of the run.\n",
        "  \"\"\"\n",
        "\n",
        "  model_helpers.apply_clean(flags.FLAGS)\n",
        "\n",
        "  # Ensures flag override logic is only executed if explicitly triggered.\n",
        "  if flags_obj.tf_gpu_thread_mode:\n",
        "    override_flags_and_set_envars_for_gpu_thread_pool(flags_obj)\n",
        "\n",
        "  # Configures cluster spec for distribution strategy.\n",
        "  num_workers = distribution_utils.configure_cluster(flags_obj.worker_hosts,\n",
        "                                                     flags_obj.task_index)\n",
        "\n",
        "  # Creates session config. allow_soft_placement = True, is required for\n",
        "  # multi-GPU and is not harmful for other modes.\n",
        "  session_config = tf.compat.v1.ConfigProto(\n",
        "      inter_op_parallelism_threads=flags_obj.inter_op_parallelism_threads,\n",
        "      intra_op_parallelism_threads=flags_obj.intra_op_parallelism_threads,\n",
        "      allow_soft_placement=True)\n",
        "\n",
        "  distribution_strategy = distribution_utils.get_distribution_strategy(\n",
        "      distribution_strategy=flags_obj.distribution_strategy,\n",
        "      num_gpus=flags_core.get_num_gpus(flags_obj),\n",
        "      num_workers=num_workers,\n",
        "      all_reduce_alg=flags_obj.all_reduce_alg)\n",
        "\n",
        "  # Creates a `RunConfig` that checkpoints every 24 hours which essentially\n",
        "  # results in checkpoints determined only by `epochs_between_evals`.\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      train_distribute=distribution_strategy,\n",
        "      session_config=session_config,\n",
        "      save_checkpoints_secs=60*60*24)\n",
        "\n",
        "  # Initializes model with all but the dense layer from pretrained ResNet.\n",
        "  if flags_obj.pretrained_model_checkpoint_path is not None:\n",
        "    warm_start_settings = tf.estimator.WarmStartSettings(\n",
        "        flags_obj.pretrained_model_checkpoint_path,\n",
        "        vars_to_warm_start='^(?!.*dense)')\n",
        "  else:\n",
        "    warm_start_settings = None\n",
        "\n",
        "  classifier = tf.estimator.Estimator(\n",
        "      model_fn=model_function, model_dir=flags_obj.model_dir, config=run_config,\n",
        "      warm_start_from=warm_start_settings, params={\n",
        "          'resnet_size': int(flags_obj.resnet_size),\n",
        "          'data_format': flags_obj.data_format,\n",
        "          'batch_size': flags_obj.batch_size,\n",
        "          'resnet_version': int(flags_obj.resnet_version),\n",
        "          'loss_scale': flags_core.get_loss_scale(flags_obj),\n",
        "          'dtype': flags_core.get_tf_dtype(flags_obj),\n",
        "          'fine_tune': flags_obj.fine_tune\n",
        "      })\n",
        "\n",
        "  run_params = {\n",
        "      'batch_size': flags_obj.batch_size,\n",
        "      'dtype': flags_core.get_tf_dtype(flags_obj),\n",
        "      'resnet_size': flags_obj.resnet_size,\n",
        "      'resnet_version': flags_obj.resnet_version,\n",
        "      'synthetic_data': flags_obj.use_synthetic_data,\n",
        "      'train_epochs': flags_obj.train_epochs,\n",
        "  }\n",
        "  if flags_obj.use_synthetic_data:\n",
        "    dataset_name = dataset_name + '-synthetic'\n",
        "\n",
        "  benchmark_logger = logger.get_benchmark_logger()\n",
        "  benchmark_logger.log_run_info('resnet', dataset_name, run_params,\n",
        "                                test_id=flags_obj.benchmark_test_id)\n",
        "\n",
        "  train_hooks = hooks_helper.get_train_hooks(\n",
        "      flags_obj.hooks,\n",
        "      model_dir=flags_obj.model_dir,\n",
        "      batch_size=flags_obj.batch_size)\n",
        "\n",
        "  def input_fn_train(num_epochs):\n",
        "    return input_function(\n",
        "        is_training=True,\n",
        "        data_dir=flags_obj.data_dir,\n",
        "        batch_size=distribution_utils.per_device_batch_size(\n",
        "            flags_obj.batch_size, flags_core.get_num_gpus(flags_obj)),\n",
        "        num_epochs=num_epochs,\n",
        "        dtype=flags_core.get_tf_dtype(flags_obj),\n",
        "        datasets_num_private_threads=flags_obj.datasets_num_private_threads,\n",
        "        num_parallel_batches=flags_obj.datasets_num_parallel_batches)\n",
        "\n",
        "  def input_fn_eval():\n",
        "    return input_function(\n",
        "        is_training=False,\n",
        "        data_dir=flags_obj.data_dir,\n",
        "        batch_size=distribution_utils.per_device_batch_size(\n",
        "            flags_obj.batch_size, flags_core.get_num_gpus(flags_obj)),\n",
        "        num_epochs=1,\n",
        "        dtype=flags_core.get_tf_dtype(flags_obj))\n",
        "\n",
        "  train_epochs = (0 if flags_obj.eval_only or not flags_obj.train_epochs else\n",
        "                  flags_obj.train_epochs)\n",
        "\n",
        "  use_train_and_evaluate = flags_obj.use_train_and_evaluate or (\n",
        "      distribution_strategy.__class__.__name__ == 'CollectiveAllReduceStrategy')\n",
        "  if use_train_and_evaluate:\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn=lambda: input_fn_train(train_epochs), hooks=train_hooks,\n",
        "        max_steps=flags_obj.max_train_steps)\n",
        "    eval_spec = tf.estimator.EvalSpec(input_fn=input_fn_eval,\n",
        "                                      steps=flags_obj.max_train_steps)\n",
        "    tf.compat.v1.logging.info('Starting to train and evaluate.')\n",
        "    eval_results, _ = tf.estimator.train_and_evaluate(classifier, train_spec,\n",
        "                                                      eval_spec)\n",
        "    benchmark_logger.log_evaluation_result(eval_results)\n",
        "  else:\n",
        "    if train_epochs == 0:\n",
        "      # If --eval_only is set, perform a single loop with zero train epochs.\n",
        "      schedule, n_loops = [0], 1\n",
        "    else:\n",
        "      # Compute the number of times to loop while training. All but the last\n",
        "      # pass will train for `epochs_between_evals` epochs, while the last will\n",
        "      # train for the number needed to reach `training_epochs`. For instance if\n",
        "      #   train_epochs = 25 and epochs_between_evals = 10\n",
        "      # schedule will be set to [10, 10, 5]. That is to say, the loop will:\n",
        "      #   Train for 10 epochs and then evaluate.\n",
        "      #   Train for another 10 epochs and then evaluate.\n",
        "      #   Train for a final 5 epochs (to reach 25 epochs) and then evaluate.\n",
        "      n_loops = math.ceil(train_epochs / flags_obj.epochs_between_evals)\n",
        "      schedule = [flags_obj.epochs_between_evals for _ in range(int(n_loops))]\n",
        "      schedule[-1] = train_epochs - sum(schedule[:-1])  # over counting.\n",
        "\n",
        "    for cycle_index, num_train_epochs in enumerate(schedule):\n",
        "      tf.compat.v1.logging.info('Starting cycle: %d/%d', cycle_index,\n",
        "                                int(n_loops))\n",
        "\n",
        "      if num_train_epochs:\n",
        "        # Since we are calling classifier.train immediately in each loop, the\n",
        "        # value of num_train_epochs in the lambda function will not be changed\n",
        "        # before it is used. So it is safe to ignore the pylint error here\n",
        "        # pylint: disable=cell-var-from-loop\n",
        "        classifier.train(input_fn=lambda: input_fn_train(num_train_epochs),\n",
        "                         hooks=train_hooks, max_steps=flags_obj.max_train_steps)\n",
        "\n",
        "      # flags_obj.max_train_steps is generally associated with testing and\n",
        "      # profiling. As a result it is frequently called with synthetic data,\n",
        "      # which will iterate forever. Passing steps=flags_obj.max_train_steps\n",
        "      # allows the eval (which is generally unimportant in those circumstances)\n",
        "      # to terminate.  Note that eval will run for max_train_steps each loop,\n",
        "      # regardless of the global_step count.\n",
        "      tf.compat.v1.logging.info('Starting to evaluate.')\n",
        "      eval_results = classifier.evaluate(input_fn=input_fn_eval,\n",
        "                                         steps=flags_obj.max_train_steps)\n",
        "\n",
        "      benchmark_logger.log_evaluation_result(eval_results)\n",
        "\n",
        "      if model_helpers.past_stop_threshold(\n",
        "          flags_obj.stop_threshold, eval_results['accuracy']):\n",
        "        break\n",
        "\n",
        "  if flags_obj.export_dir is not None:\n",
        "    # Exports a saved model for the given classifier.\n",
        "    export_dtype = flags_core.get_tf_dtype(flags_obj)\n",
        "    if flags_obj.image_bytes_as_serving_input:\n",
        "      input_receiver_fn = functools.partial(\n",
        "          image_bytes_serving_input_fn, shape, dtype=export_dtype)\n",
        "    else:\n",
        "      input_receiver_fn = export.build_tensor_serving_input_receiver_fn(\n",
        "          shape, batch_size=flags_obj.batch_size, dtype=export_dtype)\n",
        "    classifier.export_savedmodel(flags_obj.export_dir, input_receiver_fn,\n",
        "                                 strip_default_attrs=True)\n",
        "\n",
        "  stats = {}\n",
        "  stats['eval_results'] = eval_results\n",
        "  stats['train_hooks'] = train_hooks\n",
        "\n",
        "  return stats\n",
        "\n",
        "\n",
        "def define_resnet_flags(resnet_size_choices=None):\n",
        "  \"\"\"Add flags and validators for ResNet.\"\"\"\n",
        "  flags_core.define_base()\n",
        "  flags_core.define_performance(num_parallel_calls=False,\n",
        "                                tf_gpu_thread_mode=True,\n",
        "                                datasets_num_private_threads=True,\n",
        "                                datasets_num_parallel_batches=True)\n",
        "  flags_core.define_image()\n",
        "  flags_core.define_benchmark()\n",
        "  flags.adopt_module_key_flags(flags_core)\n",
        "\n",
        "  flags.DEFINE_enum(\n",
        "      name='resnet_version', short_name='rv', default='1',\n",
        "      enum_values=['1', '2'],\n",
        "      help=flags_core.help_wrap(\n",
        "          'Version of ResNet. (1 or 2) See README.md for details.'))\n",
        "  flags.DEFINE_bool(\n",
        "      name='fine_tune', short_name='ft', default=False,\n",
        "      help=flags_core.help_wrap(\n",
        "          'If True do not train any parameters except for the final layer.'))\n",
        "  flags.DEFINE_string(\n",
        "      name='pretrained_model_checkpoint_path', short_name='pmcp', default=None,\n",
        "      help=flags_core.help_wrap(\n",
        "          'If not None initialize all the network except the final layer with '\n",
        "          'these values'))\n",
        "  flags.DEFINE_boolean(\n",
        "      name='eval_only', default=False,\n",
        "      help=flags_core.help_wrap('Skip training and only perform evaluation on '\n",
        "                                'the latest checkpoint.'))\n",
        "  flags.DEFINE_boolean(\n",
        "      name='image_bytes_as_serving_input', default=False,\n",
        "      help=flags_core.help_wrap(\n",
        "          'If True exports savedmodel with serving signature that accepts '\n",
        "          'JPEG image bytes instead of a fixed size [HxWxC] tensor that '\n",
        "          'represents the image. The former is easier to use for serving at '\n",
        "          'the expense of image resize/cropping being done as part of model '\n",
        "          'inference. Note, this flag only applies to ImageNet and cannot '\n",
        "          'be used for CIFAR.'))\n",
        "  flags.DEFINE_boolean(\n",
        "      name='use_train_and_evaluate', default=False,\n",
        "      help=flags_core.help_wrap(\n",
        "          'If True, uses `tf.estimator.train_and_evaluate` for the training '\n",
        "          'and evaluation loop, instead of separate calls to `classifier.train '\n",
        "          'and `classifier.evaluate`, which is the default behavior.'))\n",
        "  flags.DEFINE_string(\n",
        "      name='worker_hosts', default=None,\n",
        "      help=flags_core.help_wrap(\n",
        "          'Comma-separated list of worker ip:port pairs for running '\n",
        "          'multi-worker models with DistributionStrategy.  The user would '\n",
        "          'start the program on each host with identical value for this flag.'))\n",
        "  flags.DEFINE_integer(\n",
        "      name='task_index', default=-1,\n",
        "      help=flags_core.help_wrap('If multi-worker training, the task_index of '\n",
        "                                'this worker.'))\n",
        "  choice_kwargs = dict(\n",
        "      name='resnet_size', short_name='rs', default='50',\n",
        "      help=flags_core.help_wrap('The size of the ResNet model to use.'))\n",
        "\n",
        "  if resnet_size_choices is None:\n",
        "    flags.DEFINE_string(**choice_kwargs)\n",
        "  else:\n",
        "    flags.DEFINE_enum(enum_values=resnet_size_choices, **choice_kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NIV0ooQe_BNN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}